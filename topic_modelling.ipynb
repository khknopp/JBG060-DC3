{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries/data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18520, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>location_article</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses the passing of the new C...</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article discusses the military actions tak...</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>Abyei</td>\n",
       "      <td>9.838551</td>\n",
       "      <td>28.486396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The article discusses the signing of a Framewo...</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Southern Kordofan</td>\n",
       "      <td>11.036544</td>\n",
       "      <td>30.895824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The article discusses the upcoming independenc...</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>6.876992</td>\n",
       "      <td>31.306979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The article discusses the need for South Sudan...</td>\n",
       "      <td>2011-07-02</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary       date  \\\n",
       "0  The article discusses the passing of the new C... 2011-07-07   \n",
       "1  The article discusses the military actions tak... 2011-07-03   \n",
       "2  The article discusses the signing of a Framewo... 2011-06-30   \n",
       "3  The article discusses the upcoming independenc... 2011-07-04   \n",
       "4  The article discusses the need for South Sudan... 2011-07-02   \n",
       "\n",
       "    location_article        lat        lng  \n",
       "0               Juba   4.859363  31.571250  \n",
       "1              Abyei   9.838551  28.486396  \n",
       "2  Southern Kordofan  11.036544  30.895824  \n",
       "3        South Sudan   6.876992  31.306979  \n",
       "4               Juba   4.859363  31.571250  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and perform preprocessing\n",
    "\n",
    "df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"]) # Read data into 'df' dataframe\n",
    "print(df.shape) # Print dataframe shape\n",
    "\n",
    "docs = df[\"summary\"].tolist() # Create a list containing all article summaries\n",
    "\n",
    "df.head() # Show first 5 dataframe entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting BERTopic\n",
    "\n",
    "This might take a while on a CPU. In the background a pre-trained Large Language Model, called the sentence embedder, is used to convert the articles to a semantic vector space. We then perform clustering in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('southsudan_model'):\n",
    "    bertopic = BERTopic.load('southsudan_model')\n",
    "else:\n",
    "    bertopic = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True) # Initialize the BERTopic model\n",
    "\n",
    "    bertopic.fit_transform(docs) # Fit the model to the list of article summaries\n",
    "    bertopic.save(\"southsudan_model\") # Save the trained model as \"southsudan_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the modularity of the model, there is a lot of randomness that hinders reproducibiity of the model.\n",
    "#To fight this, you can for example set random state in the dimensionality reduction step via the following lines \n",
    "#or explore a different approach\n",
    "\n",
    "#from bertopic import BERTopic\n",
    "#from umap import UMAP\n",
    "\n",
    "#umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "#                  min_dist=0.0, metric='cosine', random_state=42)\n",
    "#topic_model = BERTopic(umap_model=umap_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualization of the vector space\n",
    "\n",
    "As you can see, documents with related topics are close in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertopic.visualize_documents(docs) # Create a plot of the topics, this may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating smaller topics\n",
    "\n",
    "Within our list of topics, we find topics that are semantically closest to 4 keywords:\n",
    "\n",
    "\"Hunger\", \"Refugees\", \"Conflict\", and \"Humanitarian\".\n",
    "\n",
    "**Feel free to change this approach!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function to calculate a list of the top n topics related to (a) given keyword(s)\n",
    "\n",
    "def get_relevant_topics(bertopic_model, keywords, top_n):\n",
    "    '''\n",
    "    Retrieve a list of the top n number of relevant topics to the provided (list of) keyword(s)\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        bertopic_model: a (fitted) BERTopic model object\n",
    "        \n",
    "        keywords:   a string containing one or multiple keywords to match against,\n",
    "                    \n",
    "                    This can also be a list in the form of ['keyword(s)', keyword(s), ...]\n",
    "                    \n",
    "                    In this case a maximum of top_n topics will be found per list element \n",
    "                    and subsetted to the top_n most relevant topics.\n",
    "                    \n",
    "                    !!!\n",
    "                    Take care that this method only considers the relevancy per inputted keyword(s) \n",
    "                    and not the relevancy to the combined list of keywords.\n",
    "                    \n",
    "                    In other words, topics that appear in the output might be significantly related to a \n",
    "                    particular element in the list of keywords but not so to any other element, \n",
    "                    \n",
    "                    while topics that do not appear in the output might be significantly related to the \n",
    "                    combined list of keywords but not much to any of the keyword(s) in particular.\n",
    "                    !!!\n",
    "                    \n",
    "        top_n: an integer indicating the number of desired relevant topics to be retrieved\n",
    "        \n",
    "        \n",
    "        Return: a list of the top_n (or less) topics most relevant to the (list of) provided keyword(s)\n",
    "    '''\n",
    "    \n",
    "    if type(keywords) is str: keywords = [keywords] # If a single string is provided convert it to list type\n",
    "    \n",
    "    relevant_topics = list() # Initilize an empty list of relevant topics\n",
    "    \n",
    "    for keyword in keywords: # Iterate through list of keywords\n",
    "        \n",
    "        # Find the top n number of topics related to the current keyword(s)\n",
    "        topics = bertopic_model.find_topics(keyword, top_n = top_n)\n",
    "        \n",
    "        # Add the topics to the list of relevant topics in the form of (topic_id, relevancy)\n",
    "        relevant_topics.extend(\n",
    "            zip(topics[0], topics[1]) # topics[0] = topic_id, topics[1] = relevancy\n",
    "        )\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1]) # Sort the list of topics on ASCENDING ORDER of relevancy\n",
    "    \n",
    "    # Get a list of the set of unique topics (with greates relevancy in case of duplicate topics)\n",
    "    relevant_topics = list(dict(relevant_topics).items())\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1], reverse=True) # Now sort the list of topics on DESCENDING ORDER of relevancy\n",
    "    \n",
    "    return relevant_topics[:10] # Return a list of the top_n unique relevant topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of articles that do not get sorted into either of the categories. So, feel free to change or expand this approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for food related categories : food insecurity previously 370 now 553\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>location_article</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses the passing of the new C...</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article discusses the military actions tak...</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>Abyei</td>\n",
       "      <td>9.838551</td>\n",
       "      <td>28.486396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The article discusses the signing of a Framewo...</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Southern Kordofan</td>\n",
       "      <td>11.036544</td>\n",
       "      <td>30.895824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The article discusses the upcoming independenc...</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>6.876992</td>\n",
       "      <td>31.306979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The article discusses the need for South Sudan...</td>\n",
       "      <td>2011-07-02</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18515</th>\n",
       "      <td>The article discusses the successful delivery ...</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>Maiwut Primary Health Care Centre</td>\n",
       "      <td>8.606200</td>\n",
       "      <td>33.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18516</th>\n",
       "      <td>The article discusses the bombing and forced e...</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>Khartoum</td>\n",
       "      <td>15.500654</td>\n",
       "      <td>32.559899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18517</th>\n",
       "      <td>The article discusses how Prime Minister Abiy ...</td>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>Addis Ababa</td>\n",
       "      <td>8.980603</td>\n",
       "      <td>38.757761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18518</th>\n",
       "      <td>The article discusses the collapse of a commer...</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>Kampala International University</td>\n",
       "      <td>0.294360</td>\n",
       "      <td>32.603970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>The article discusses the establishment of a m...</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>North Darfur State</td>\n",
       "      <td>15.766197</td>\n",
       "      <td>24.904221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18520 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 summary       date  \\\n",
       "0      The article discusses the passing of the new C... 2011-07-07   \n",
       "1      The article discusses the military actions tak... 2011-07-03   \n",
       "2      The article discusses the signing of a Framewo... 2011-06-30   \n",
       "3      The article discusses the upcoming independenc... 2011-07-04   \n",
       "4      The article discusses the need for South Sudan... 2011-07-02   \n",
       "...                                                  ...        ...   \n",
       "18515  The article discusses the successful delivery ... 2023-04-26   \n",
       "18516  The article discusses the bombing and forced e... 2023-04-26   \n",
       "18517  The article discusses how Prime Minister Abiy ... 2023-04-23   \n",
       "18518  The article discusses the collapse of a commer... 2023-04-17   \n",
       "18519  The article discusses the establishment of a m... 2023-04-24   \n",
       "\n",
       "                        location_article        lat        lng  \n",
       "0                                   Juba   4.859363  31.571250  \n",
       "1                                  Abyei   9.838551  28.486396  \n",
       "2                      Southern Kordofan  11.036544  30.895824  \n",
       "3                            South Sudan   6.876992  31.306979  \n",
       "4                                   Juba   4.859363  31.571250  \n",
       "...                                  ...        ...        ...  \n",
       "18515  Maiwut Primary Health Care Centre   8.606200  33.924100  \n",
       "18516                           Khartoum  15.500654  32.559899  \n",
       "18517                        Addis Ababa   8.980603  38.757761  \n",
       "18518   Kampala International University   0.294360  32.603970  \n",
       "18519                 North Darfur State  15.766197  24.904221  \n",
       "\n",
       "[18520 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_newtopics = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"]) # Read data into 'df' dataframe\n",
    "df_newtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_crisis_keywords = [\n",
    "    \"Food crisis\", \"Hunger\", \"Starvation\", \"Malnutrition\", \n",
    "    \"Famine\", \"Food scarcity\", \"Food insecurity\", \"Undernourished\", \n",
    "    \"Food shortage\", \"Hunger strike\", \"Food aid\", \"Emergency relief\", \n",
    "    \"Nutritional deficiency\", \"Food distribution\", \"Food bank\", \n",
    "    \"Food desert\", \"Agricultural collapse\", \"Rising food prices\", \n",
    "    \"Food rationing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 0.630256\n",
      "17 0.6161678\n",
      "80 0.5772756\n",
      "181 0.57381785\n",
      "115 0.5703357\n",
      "168 0.55105084\n",
      "27 0.51323843\n",
      "228 0.46964934\n",
      "226 0.46925715\n",
      "3 0.46828657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>location_article</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>hunger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses the passing of the new C...</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article discusses the military actions tak...</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>Abyei</td>\n",
       "      <td>9.838551</td>\n",
       "      <td>28.486396</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The article discusses the signing of a Framewo...</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Southern Kordofan</td>\n",
       "      <td>11.036544</td>\n",
       "      <td>30.895824</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The article discusses the upcoming independenc...</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>6.876992</td>\n",
       "      <td>31.306979</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The article discusses the need for South Sudan...</td>\n",
       "      <td>2011-07-02</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18515</th>\n",
       "      <td>The article discusses the successful delivery ...</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>Maiwut Primary Health Care Centre</td>\n",
       "      <td>8.606200</td>\n",
       "      <td>33.924100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18516</th>\n",
       "      <td>The article discusses the bombing and forced e...</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>Khartoum</td>\n",
       "      <td>15.500654</td>\n",
       "      <td>32.559899</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18517</th>\n",
       "      <td>The article discusses how Prime Minister Abiy ...</td>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>Addis Ababa</td>\n",
       "      <td>8.980603</td>\n",
       "      <td>38.757761</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18518</th>\n",
       "      <td>The article discusses the collapse of a commer...</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>Kampala International University</td>\n",
       "      <td>0.294360</td>\n",
       "      <td>32.603970</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>The article discusses the establishment of a m...</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>North Darfur State</td>\n",
       "      <td>15.766197</td>\n",
       "      <td>24.904221</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18520 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 summary       date  \\\n",
       "0      The article discusses the passing of the new C... 2011-07-07   \n",
       "1      The article discusses the military actions tak... 2011-07-03   \n",
       "2      The article discusses the signing of a Framewo... 2011-06-30   \n",
       "3      The article discusses the upcoming independenc... 2011-07-04   \n",
       "4      The article discusses the need for South Sudan... 2011-07-02   \n",
       "...                                                  ...        ...   \n",
       "18515  The article discusses the successful delivery ... 2023-04-26   \n",
       "18516  The article discusses the bombing and forced e... 2023-04-26   \n",
       "18517  The article discusses how Prime Minister Abiy ... 2023-04-23   \n",
       "18518  The article discusses the collapse of a commer... 2023-04-17   \n",
       "18519  The article discusses the establishment of a m... 2023-04-24   \n",
       "\n",
       "                        location_article        lat        lng  hunger  \n",
       "0                                   Juba   4.859363  31.571250   False  \n",
       "1                                  Abyei   9.838551  28.486396   False  \n",
       "2                      Southern Kordofan  11.036544  30.895824   False  \n",
       "3                            South Sudan   6.876992  31.306979   False  \n",
       "4                                   Juba   4.859363  31.571250   False  \n",
       "...                                  ...        ...        ...     ...  \n",
       "18515  Maiwut Primary Health Care Centre   8.606200  33.924100   False  \n",
       "18516                           Khartoum  15.500654  32.559899   False  \n",
       "18517                        Addis Ababa   8.980603  38.757761   False  \n",
       "18518   Kampala International University   0.294360  32.603970   False  \n",
       "18519                 North Darfur State  15.766197  24.904221   False  \n",
       "\n",
       "[18520 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords 'hunger' and 'food insecurity'\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=food_crisis_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"hunger\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['hunger']==True].count()\n",
    "df_newtopics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for conflict and violence result previously 223 now 396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_and_violence_keywords = [\n",
    "    \"Conflict\", \"Violence\", \"War\", \"Battle\", \"Skirmish\", \"Attack\", \n",
    "    \"Terrorism\", \"Militant\", \"Insurgency\", \"Riot\", \"Clashes\", \"Strife\",\n",
    "    \"Aggression\", \"Hostilities\", \"Bombing\", \"Shooting\", \"Casualties\", \n",
    "    \"Armed\", \"Rebellion\", \"Assault\", \"Combat\", \"Siege\", \"Civil war\", \n",
    "    \"Ethnic tension\", \"Genocide\", \"Massacre\", \"Hostage\", \"Guerrilla\", \n",
    "    \"Coup\", \"Uprising\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 0.5245111\n",
      "70 0.5057998\n",
      "172 0.5053438\n",
      "195 0.5013187\n",
      "106 0.48698074\n",
      "53 0.47766948\n",
      "116 0.47572848\n",
      "218 0.4688604\n",
      "60 0.4616946\n",
      "187 0.45715407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             329\n",
       "date                329\n",
       "location_article    329\n",
       "lat                 329\n",
       "lng                 329\n",
       "hunger              329\n",
       "conflict            329\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords conflict and violence\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=conflict_and_violence_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"conflict\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['conflict']==True].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for humanitarian 634 then 643 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanitarian_aid_keywords = [\n",
    "    \"Food aid\", \"Emergency relief\", \"Aid convoy\", \"Donations\", \n",
    "    \"Non-governmental organizations\", \"Charities\", \"Intervention\", \n",
    "    \"Supplies\", \"Medical aid\", \"Humanitarian\", \"Help\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 0.6475625\n",
      "3 0.64314044\n",
      "208 0.64294755\n",
      "85 0.630256\n",
      "53 0.6127999\n",
      "28 0.60199976\n",
      "228 0.5950663\n",
      "114 0.5948111\n",
      "186 0.5924746\n",
      "2 0.5801154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             819\n",
       "date                819\n",
       "location_article    819\n",
       "lat                 819\n",
       "lng                 819\n",
       "hunger              819\n",
       "conflict            819\n",
       "humanitarian        819\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords humanitarian aid\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=humanitarian_aid_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"humanitarian\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['humanitarian']==True].count()\n",
    "#list(df_newtopics['summary'][df_newtopics['humanitarian']==True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for forced displacements / refugees then now 493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_displacements_keywords = [\n",
    "    \"Refugees\", \"Internally displaced\", \"Eviction\", \"Migration\",\n",
    "    \"Relocation\", \"Camp\", \"Asylum seekers\", \"Exodus\", \n",
    "    \"Population displacement\", \"Border crossing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.6876225\n",
      "2 0.6605958\n",
      "241 0.64660645\n",
      "138 0.6259512\n",
      "239 0.5960009\n",
      "28 0.57727945\n",
      "186 0.54077786\n",
      "228 0.5366763\n",
      "47 0.5160629\n",
      "130 0.5128032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>137</td>\n",
       "      <td>15_refugees_uganda_refugee_district</td>\n",
       "      <td>[refugees, uganda, refugee, district, adjumani...</td>\n",
       "      <td>[The article discusses the launch of a regiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285</td>\n",
       "      <td>2_refugees_unhcr_refugee_nile</td>\n",
       "      <td>[refugees, unhcr, refugee, nile, yida, camp, c...</td>\n",
       "      <td>[The article discusses the increasing number o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>11</td>\n",
       "      <td>241_israeli_israel_migrants_immigrants</td>\n",
       "      <td>[israeli, israel, migrants, immigrants, asylum...</td>\n",
       "      <td>[The article discusses an incident where Egypt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>27</td>\n",
       "      <td>138_kakuma_refugee_camp_kenya</td>\n",
       "      <td>[kakuma, refugee, camp, kenya, refugees, camps...</td>\n",
       "      <td>[The article discusses the influx of refugees ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>12</td>\n",
       "      <td>239_darfur_chad_darfuri_tissi</td>\n",
       "      <td>[darfur, chad, darfuri, tissi, displaced, retu...</td>\n",
       "      <td>[The article discusses the influx of Chadian r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100</td>\n",
       "      <td>28_displaced_idps_people_internally</td>\n",
       "      <td>[displaced, idps, people, internally, bases, u...</td>\n",
       "      <td>[The article discusses violent activities amon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>18</td>\n",
       "      <td>186_civilians_unmiss_un_bases</td>\n",
       "      <td>[civilians, unmiss, un, bases, refuge, displac...</td>\n",
       "      <td>[The article discusses new fighting in South S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>12</td>\n",
       "      <td>228_bentiu_base_flooding_drinking</td>\n",
       "      <td>[bentiu, base, flooding, drinking, sanitation,...</td>\n",
       "      <td>[The article discusses the horrific living con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>77</td>\n",
       "      <td>47_returnees_kosti_iom_repatriation</td>\n",
       "      <td>[returnees, kosti, iom, repatriation, stranded...</td>\n",
       "      <td>[The article discusses the arrival of the last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>28</td>\n",
       "      <td>130_kenyans_evacuation_kenyan_nationals</td>\n",
       "      <td>[kenyans, evacuation, kenyan, nationals, fligh...</td>\n",
       "      <td>[The article discusses the evacuation of Kenya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Count                                     Name  \\\n",
       "Topic                                                   \n",
       "15       137      15_refugees_uganda_refugee_district   \n",
       "2        285            2_refugees_unhcr_refugee_nile   \n",
       "241       11   241_israeli_israel_migrants_immigrants   \n",
       "138       27            138_kakuma_refugee_camp_kenya   \n",
       "239       12            239_darfur_chad_darfuri_tissi   \n",
       "28       100      28_displaced_idps_people_internally   \n",
       "186       18            186_civilians_unmiss_un_bases   \n",
       "228       12        228_bentiu_base_flooding_drinking   \n",
       "47        77      47_returnees_kosti_iom_repatriation   \n",
       "130       28  130_kenyans_evacuation_kenyan_nationals   \n",
       "\n",
       "                                          Representation  \\\n",
       "Topic                                                      \n",
       "15     [refugees, uganda, refugee, district, adjumani...   \n",
       "2      [refugees, unhcr, refugee, nile, yida, camp, c...   \n",
       "241    [israeli, israel, migrants, immigrants, asylum...   \n",
       "138    [kakuma, refugee, camp, kenya, refugees, camps...   \n",
       "239    [darfur, chad, darfuri, tissi, displaced, retu...   \n",
       "28     [displaced, idps, people, internally, bases, u...   \n",
       "186    [civilians, unmiss, un, bases, refuge, displac...   \n",
       "228    [bentiu, base, flooding, drinking, sanitation,...   \n",
       "47     [returnees, kosti, iom, repatriation, stranded...   \n",
       "130    [kenyans, evacuation, kenyan, nationals, fligh...   \n",
       "\n",
       "                                     Representative_Docs  \n",
       "Topic                                                     \n",
       "15     [The article discusses the launch of a regiona...  \n",
       "2      [The article discusses the increasing number o...  \n",
       "241    [The article discusses an incident where Egypt...  \n",
       "138    [The article discusses the influx of refugees ...  \n",
       "239    [The article discusses the influx of Chadian r...  \n",
       "28     [The article discusses violent activities amon...  \n",
       "186    [The article discusses new fighting in South S...  \n",
       "228    [The article discusses the horrific living con...  \n",
       "47     [The article discusses the arrival of the last...  \n",
       "130    [The article discusses the evacuation of Kenya...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords refugees\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=forced_displacements_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"refugees\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "#df_newtopics[df_newtopics['refugees']==True].count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional new categories\n",
    "Top 10 categories for political instability NOW: 515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_instability_keywords = [\n",
    "    \"Coup\", \"Unrest\", \"Regime change\", \"Sanctions\", \"Governance\", \n",
    "    \"Corruption\", \"Protests\", \"Civil unrest\", \"Election\", \"Political crisis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 0.6067066\n",
      "8 0.58838606\n",
      "76 0.5493268\n",
      "20 0.52059174\n",
      "70 0.5166007\n",
      "108 0.51253843\n",
      "195 0.5011233\n",
      "72 0.4988699\n",
      "131 0.49361986\n",
      "116 0.49328315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             618\n",
       "date                618\n",
       "location_article    618\n",
       "lat                 618\n",
       "lng                 618\n",
       "hunger              618\n",
       "conflict            618\n",
       "humanitarian        618\n",
       "refugees            618\n",
       "politics            618\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords political instability\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=political_instability_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"politics\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['politics']==True].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for economic issues NOW 476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "economic_issues_keywords = [\n",
    "    \"Recession\", \"Inflation\", \"Unemployment\", \"Economic collapse\", \n",
    "    \"Debt\", \"Trade barrier\", \"Currency devaluation\", \"Financial crisis\",\n",
    "    \"Poverty\", \"Economic sanctions\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 0.5655854\n",
      "34 0.56004345\n",
      "17 0.4726956\n",
      "102 0.45058596\n",
      "160 0.44329643\n",
      "115 0.42803964\n",
      "181 0.42509422\n",
      "43 0.41583616\n",
      "194 0.41398948\n",
      "80 0.41186297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             543\n",
       "date                543\n",
       "location_article    543\n",
       "lat                 543\n",
       "lng                 543\n",
       "hunger              543\n",
       "conflict            543\n",
       "humanitarian        543\n",
       "refugees            543\n",
       "politics            543\n",
       "economics           543\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords economic issues\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=economic_issues_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"economics\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['economics']==True].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for production shortage NOW 489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_shortage_keywords = [\n",
    "    \"Crop failure\", \"Supply chain disruption\", \"Agriculture decline\",\n",
    "    \"Harvest\", \"Irrigation problems\", \"Yield\", \"Production halt\",\n",
    "    \"Shortfall\", \"Agriculture technology\", \"Infrastructure\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0.55947334\n",
      "246 0.46831745\n",
      "80 0.4261831\n",
      "129 0.42516935\n",
      "179 0.4201357\n",
      "17 0.4173216\n",
      "119 0.40326\n",
      "63 0.3929451\n",
      "181 0.39060515\n",
      "219 0.38318223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             450\n",
       "date                450\n",
       "location_article    450\n",
       "lat                 450\n",
       "lng                 450\n",
       "hunger              450\n",
       "conflict            450\n",
       "humanitarian        450\n",
       "refugees            450\n",
       "politics            450\n",
       "economics           450\n",
       "production          450\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords production shortage\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=production_shortage_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"production\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['production']==True].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for Land Related Issues NOW 474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_related_issues_keywords = [\n",
    "    \"Land rights\", \"Land grab\", \"Land degradation\", \"Land reform\", \n",
    "    \"Deforestation\", \"Afforestation\", \"Land dispute\", \"Territorial disputes\",\n",
    "    \"Land tenure\", \"Land use change\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 0.5959922\n",
      "112 0.46228904\n",
      "234 0.45230207\n",
      "195 0.4332926\n",
      "89 0.43215275\n",
      "67 0.4220714\n",
      "99 0.40830228\n",
      "78 0.3916825\n",
      "68 0.38830143\n",
      "44 0.3851289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             394\n",
       "date                394\n",
       "location_article    394\n",
       "lat                 394\n",
       "lng                 394\n",
       "hunger              394\n",
       "conflict            394\n",
       "humanitarian        394\n",
       "refugees            394\n",
       "politics            394\n",
       "economics           394\n",
       "production          394\n",
       "land                394\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords land related issues\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=land_related_issues_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"land\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['land']==True].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 categories for pests and diseases NOW 462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pests_and_diseases_keywords = [\n",
    "    \"Pest infestation\", \"Crop disease\", \"Blights\", \"Locust\", \n",
    "    \"Weevils\", \"Molds\", \"Fungus\", \"Agricultural pests\", \"Pathogens\", \n",
    "    \"Invasive species\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 0.4810661\n",
      "40 0.41487882\n",
      "27 0.40696007\n",
      "80 0.3913884\n",
      "144 0.38963416\n",
      "41 0.3808151\n",
      "17 0.37353253\n",
      "66 0.37348843\n",
      "123 0.36794478\n",
      "179 0.36504847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             590\n",
       "date                590\n",
       "location_article    590\n",
       "lat                 590\n",
       "lng                 590\n",
       "hunger              590\n",
       "conflict            590\n",
       "humanitarian        590\n",
       "refugees            590\n",
       "politics            590\n",
       "economics           590\n",
       "production          590\n",
       "land                590\n",
       "pests               590\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords pests and diseases\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=pests_and_diseases_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"pests\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['pests']==True].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmental_issues_keywords = [\n",
    "    \"Drought\", \"Climate change\", \"Flood\", \"Desertification\", \n",
    "    \"Natural disasters\", \"Water scarcity\", \"Heatwave\", \"Sea level rise\", \n",
    "    \"Erosion\", \"Pollution\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 0.5480695\n",
      "44 0.53715414\n",
      "228 0.4821716\n",
      "193 0.41341066\n",
      "86 0.41012728\n",
      "28 0.38149956\n",
      "134 0.3732643\n",
      "17 0.37106913\n",
      "145 0.36905771\n",
      "245 0.36735898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(129, 0.5480695),\n",
       " (44, 0.53715414),\n",
       " (228, 0.4821716),\n",
       " (193, 0.41341066),\n",
       " (86, 0.41012728),\n",
       " (28, 0.38149956),\n",
       " (134, 0.3732643),\n",
       " (17, 0.37106913),\n",
       " (145, 0.36905771),\n",
       " (245, 0.36735898)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords enviromental issues\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=environmental_issues_keywords, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"environment\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['environment']==True].count()\n",
    "\n",
    "relevant_topics\n",
    "# topic_terms = bertopic.get_topic(128)\n",
    "# topic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Diseases 447\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_dieseases = [\n",
    "    \"covid\",\n",
    "    \"cholera\",\n",
    "    \"malaria\",\n",
    "    \"hiv\",\n",
    "    \"aids\",\n",
    "    \"ebola\",\n",
    "    \"tyfus\",\n",
    "    \"odra\",\n",
    "    \"hepatitis\",\n",
    "    \"monkeypox\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 0.7132701\n",
      "123 0.6793604\n",
      "40 0.6528835\n",
      "111 0.6517547\n",
      "41 0.55072457\n",
      "191 0.5271292\n",
      "211 0.42946506\n",
      "66 0.41168702\n",
      "46 0.35681075\n",
      "65 0.35555172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary             474\n",
       "date                474\n",
       "location_article    474\n",
       "lat                 474\n",
       "lng                 474\n",
       "hunger              474\n",
       "conflict            474\n",
       "humanitarian        474\n",
       "refugees            474\n",
       "politics            474\n",
       "economics           474\n",
       "production          474\n",
       "land                474\n",
       "pests               474\n",
       "environment         474\n",
       "major               474\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 topics related to the keywords enviromental issues\n",
    "\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic, keywords=major_dieseases, top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df_newtopics[\"major\"] = [t in topic_ids for t in bertopic.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "df_newtopics[df_newtopics['major']==True].count()\n",
    "\n",
    "\n",
    "# topic_terms = bertopic.get_topic(111)\n",
    "# topic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW TOTAL 3526 WITH AT LEAST ONE CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_newtopics.to_csv(\"data/articles_newtopics.csv\", index=False) # Save DataFrame to articles_newtopics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 summary       date  \\\n",
      "11     The article discusses the South Sudan governme... 2011-07-03   \n",
      "18     The article discusses the potential for full-s... 2011-07-07   \n",
      "45     The article discusses the ongoing conflict and... 2011-07-02   \n",
      "48     The article discusses the decision of Western ... 2011-06-30   \n",
      "80     The article discusses the difficulties faced b... 2011-07-05   \n",
      "...                                                  ...        ...   \n",
      "18493  The article discusses the denial by the govern... 2023-02-10   \n",
      "18496  The article discusses how conflict, political ... 2023-01-30   \n",
      "18506  The article discusses how armed raids in Great... 2022-12-30   \n",
      "18516  The article discusses the bombing and forced e... 2023-04-26   \n",
      "18519  The article discusses the establishment of a m... 2023-04-24   \n",
      "\n",
      "                        location_article        lat        lng  hunger  \\\n",
      "11                                  Juba   4.859363  31.571250   False   \n",
      "18                                 Abyei   9.838551  28.486396   False   \n",
      "45                        South Kordofan  11.036544  30.895824   False   \n",
      "48                         Yambio County   4.891862  28.486396    True   \n",
      "80                                 Kosti  13.123468  32.650351   False   \n",
      "...                                  ...        ...        ...     ...   \n",
      "18493                            Nadapal   4.406781  34.283832   False   \n",
      "18496                            Nairobi  -1.292066  36.821946    True   \n",
      "18506  Greater Pibor Administrative Area   6.217554  33.438353   False   \n",
      "18516                           Khartoum  15.500654  32.559899   False   \n",
      "18519                 North Darfur State  15.766197  24.904221   False   \n",
      "\n",
      "       conflict  humanitarian  refugees  politics  economics  production  \\\n",
      "11        False         False     False     False      False       False   \n",
      "18        False         False     False     False      False       False   \n",
      "45         True         False     False     False      False       False   \n",
      "48        False         False     False     False      False        True   \n",
      "80        False         False      True     False      False       False   \n",
      "...         ...           ...       ...       ...        ...         ...   \n",
      "18493     False         False     False     False      False       False   \n",
      "18496     False         False     False     False       True       False   \n",
      "18506     False         False     False      True      False       False   \n",
      "18516     False         False     False     False      False       False   \n",
      "18519      True         False     False     False      False       False   \n",
      "\n",
      "        land  pests  environment  major  \n",
      "11     False  False         True  False  \n",
      "18      True  False        False  False  \n",
      "45     False  False        False  False  \n",
      "48     False   True        False  False  \n",
      "80     False  False        False  False  \n",
      "...      ...    ...          ...    ...  \n",
      "18493   True  False        False  False  \n",
      "18496  False  False        False  False  \n",
      "18506  False  False        False  False  \n",
      "18516  False  False        False   True  \n",
      "18519  False  False        False  False  \n",
      "\n",
      "[3567 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df_newtopics[df_newtopics.applymap(lambda x: x == True).any(axis=1)]\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 629/629 [00:00<?, ?B/s] \n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:10<00:00, 26.5MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 48.5kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.36MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\topic_modelling.ipynb Cell 53\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mresult[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m df_newtopics[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_newtopics[\u001b[39m'\u001b[39;49m\u001b[39msummary\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(get_sentiment_score)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# 3. Trend Analysis\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m monthly_sentiments \u001b[39m=\u001b[39m df_newtopics\u001b[39m.\u001b[39mgroupby([df_newtopics[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mto_period(\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39munstack()\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1144\u001b[0m             values,\n\u001b[0;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\topic_modelling.ipynb Cell 53\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sentiment_score\u001b[39m(text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     result \u001b[39m=\u001b[39m analyzer(text)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPOSITIVE\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/20211445/OneDrive%20-%20TU%20Eindhoven/Desktop/University/Y3Q1/DBL%203/JBG060-DC3/topic_modelling.ipynb#Y103sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:789\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    787\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 789\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m    790\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    791\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    792\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    793\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    794\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    795\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    796\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    797\u001b[0m )\n\u001b[0;32m    798\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    799\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:609\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    605\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    607\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    610\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    611\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    612\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    613\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    614\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    615\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    616\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:375\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    368\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    369\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    370\u001b[0m         hidden_state,\n\u001b[0;32m    371\u001b[0m         attn_mask,\n\u001b[0;32m    372\u001b[0m         head_mask[i],\n\u001b[0;32m    373\u001b[0m     )\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 375\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    376\u001b[0m         hidden_state,\n\u001b[0;32m    377\u001b[0m         attn_mask,\n\u001b[0;32m    378\u001b[0m         head_mask[i],\n\u001b[0;32m    379\u001b[0m         output_attentions,\n\u001b[0;32m    380\u001b[0m     )\n\u001b[0;32m    382\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\20211445\\OneDrive - TU Eindhoven\\Desktop\\University\\Y3Q1\\DBL 3\\JBG060-DC3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:220\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    217\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    219\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[0;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[0;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Topic Modeling\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "data_vectorized = vectorizer.fit_transform(df_newtopics['summary'])\n",
    "\n",
    "# Create a gensim dictionary and ensure summaries are strings and then tokenize them\n",
    "df_newtopics['tokens'] = df_newtopics['summary'].apply(lambda x: str(x).split())\n",
    "dictionary = Dictionary(df_newtopics['tokens'])\n",
    "\n",
    "# Build the corpus using the tokens\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df_newtopics['tokens']]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=30)\n",
    "\n",
    "def get_main_topic(tokens):\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    main_topic = max(lda_model[bow], key=lambda tup: tup[1])\n",
    "    return main_topic[0]\n",
    "\n",
    "df_newtopics['topic'] = df_newtopics['tokens'].apply(get_main_topic)\n",
    "\n",
    "# 2. Sentiment Analysis\n",
    "analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    result = analyzer(text)[0]\n",
    "    if result['label'] == 'POSITIVE':\n",
    "        return result['score']\n",
    "    else:\n",
    "        return -result['score']\n",
    "\n",
    "df_newtopics['sentiment'] = df_newtopics['summary'].apply(get_sentiment_score)\n",
    "\n",
    "# 3. Trend Analysis\n",
    "monthly_sentiments = df_newtopics.groupby([df_newtopics['date'].dt.to_period('M'), 'topic'])['sentiment'].mean().unstack()\n",
    "\n",
    "print(monthly_sentiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below is specific for the sentiment analysis above therefore it requires some modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>6</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>19</th>\n",
       "      <th>25</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>1_diff</th>\n",
       "      <th>2_diff</th>\n",
       "      <th>6_diff</th>\n",
       "      <th>14_diff</th>\n",
       "      <th>15_diff</th>\n",
       "      <th>19_diff</th>\n",
       "      <th>25_diff</th>\n",
       "      <th>27_diff</th>\n",
       "      <th>28_diff</th>\n",
       "      <th>29_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-06</th>\n",
       "      <td>0.688175</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>0.231677</td>\n",
       "      <td>0.605170</td>\n",
       "      <td>-0.317350</td>\n",
       "      <td>-0.254239</td>\n",
       "      <td>0.370362</td>\n",
       "      <td>-0.450529</td>\n",
       "      <td>-0.414348</td>\n",
       "      <td>-0.551050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07</th>\n",
       "      <td>0.613761</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.316726</td>\n",
       "      <td>0.657156</td>\n",
       "      <td>-0.328491</td>\n",
       "      <td>0.267233</td>\n",
       "      <td>0.370362</td>\n",
       "      <td>-0.473375</td>\n",
       "      <td>-0.414348</td>\n",
       "      <td>-0.979257</td>\n",
       "      <td>-0.074414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.548403</td>\n",
       "      <td>0.051985</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>0.521472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.428207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08</th>\n",
       "      <td>0.729675</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.523801</td>\n",
       "      <td>0.463213</td>\n",
       "      <td>-0.357604</td>\n",
       "      <td>0.036319</td>\n",
       "      <td>0.370362</td>\n",
       "      <td>-0.980567</td>\n",
       "      <td>-0.414348</td>\n",
       "      <td>-0.139213</td>\n",
       "      <td>0.115913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.207075</td>\n",
       "      <td>-0.193943</td>\n",
       "      <td>-0.029113</td>\n",
       "      <td>-0.230914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.507192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09</th>\n",
       "      <td>0.585312</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>0.234096</td>\n",
       "      <td>0.419113</td>\n",
       "      <td>-0.554267</td>\n",
       "      <td>0.466481</td>\n",
       "      <td>0.370362</td>\n",
       "      <td>-0.980567</td>\n",
       "      <td>-0.414348</td>\n",
       "      <td>-0.327074</td>\n",
       "      <td>-0.144363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.757897</td>\n",
       "      <td>-0.044099</td>\n",
       "      <td>-0.196663</td>\n",
       "      <td>0.430162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.187861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10</th>\n",
       "      <td>0.835639</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.197548</td>\n",
       "      <td>0.403372</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.080131</td>\n",
       "      <td>-0.744252</td>\n",
       "      <td>-0.191858</td>\n",
       "      <td>-0.414348</td>\n",
       "      <td>-0.630267</td>\n",
       "      <td>0.250327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.431644</td>\n",
       "      <td>-0.015742</td>\n",
       "      <td>0.556933</td>\n",
       "      <td>-0.386351</td>\n",
       "      <td>-1.114613</td>\n",
       "      <td>0.788709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.303192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12</th>\n",
       "      <td>0.986017</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.987557</td>\n",
       "      <td>0.688767</td>\n",
       "      <td>-0.922577</td>\n",
       "      <td>-0.997625</td>\n",
       "      <td>0.868274</td>\n",
       "      <td>-0.994578</td>\n",
       "      <td>-0.996758</td>\n",
       "      <td>0.608692</td>\n",
       "      <td>-0.010003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.069210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.568919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01</th>\n",
       "      <td>0.358064</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.987557</td>\n",
       "      <td>0.333655</td>\n",
       "      <td>0.935065</td>\n",
       "      <td>-0.996539</td>\n",
       "      <td>0.868274</td>\n",
       "      <td>-0.994578</td>\n",
       "      <td>-0.996758</td>\n",
       "      <td>0.608692</td>\n",
       "      <td>-0.627953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.355113</td>\n",
       "      <td>1.857642</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.224990</td>\n",
       "      <td>0.491596</td>\n",
       "      <td>0.941952</td>\n",
       "      <td>-0.996539</td>\n",
       "      <td>0.868274</td>\n",
       "      <td>-0.994578</td>\n",
       "      <td>-0.996758</td>\n",
       "      <td>-0.741725</td>\n",
       "      <td>-0.357666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.762566</td>\n",
       "      <td>0.157941</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.350417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03</th>\n",
       "      <td>0.934713</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.224990</td>\n",
       "      <td>0.612692</td>\n",
       "      <td>-0.986989</td>\n",
       "      <td>-0.943442</td>\n",
       "      <td>0.868274</td>\n",
       "      <td>-0.994578</td>\n",
       "      <td>-0.996758</td>\n",
       "      <td>-0.741725</td>\n",
       "      <td>0.934315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121096</td>\n",
       "      <td>-1.928941</td>\n",
       "      <td>0.053098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04</th>\n",
       "      <td>0.342806</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>-0.916847</td>\n",
       "      <td>0.372461</td>\n",
       "      <td>-0.921591</td>\n",
       "      <td>0.933351</td>\n",
       "      <td>0.868274</td>\n",
       "      <td>-0.994578</td>\n",
       "      <td>-0.996758</td>\n",
       "      <td>-0.741725</td>\n",
       "      <td>-0.591908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.691856</td>\n",
       "      <td>-0.240231</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>1.876793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "topic           1         2         6        14        15        19        25  \\\n",
       "date                                                                            \n",
       "2011-06  0.688175  0.800203  0.231677  0.605170 -0.317350 -0.254239  0.370362   \n",
       "2011-07  0.613761  0.800203 -0.316726  0.657156 -0.328491  0.267233  0.370362   \n",
       "2011-08  0.729675  0.800203 -0.523801  0.463213 -0.357604  0.036319  0.370362   \n",
       "2011-09  0.585312  0.800203  0.234096  0.419113 -0.554267  0.466481  0.370362   \n",
       "2011-10  0.835639  0.800203 -0.197548  0.403372  0.002666  0.080131 -0.744252   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "2022-12  0.986017  0.800203 -0.987557  0.688767 -0.922577 -0.997625  0.868274   \n",
       "2023-01  0.358064  0.800203 -0.987557  0.333655  0.935065 -0.996539  0.868274   \n",
       "2023-02  0.000399  0.800203 -0.224990  0.491596  0.941952 -0.996539  0.868274   \n",
       "2023-03  0.934713  0.800203 -0.224990  0.612692 -0.986989 -0.943442  0.868274   \n",
       "2023-04  0.342806  0.800203 -0.916847  0.372461 -0.921591  0.933351  0.868274   \n",
       "\n",
       "topic          27        28        29    1_diff  2_diff    6_diff   14_diff  \\\n",
       "date                                                                          \n",
       "2011-06 -0.450529 -0.414348 -0.551050       NaN     NaN       NaN       NaN   \n",
       "2011-07 -0.473375 -0.414348 -0.979257 -0.074414     0.0 -0.548403  0.051985   \n",
       "2011-08 -0.980567 -0.414348 -0.139213  0.115913     0.0 -0.207075 -0.193943   \n",
       "2011-09 -0.980567 -0.414348 -0.327074 -0.144363     0.0  0.757897 -0.044099   \n",
       "2011-10 -0.191858 -0.414348 -0.630267  0.250327     0.0 -0.431644 -0.015742   \n",
       "...           ...       ...       ...       ...     ...       ...       ...   \n",
       "2022-12 -0.994578 -0.996758  0.608692 -0.010003     0.0  0.000000  0.488800   \n",
       "2023-01 -0.994578 -0.996758  0.608692 -0.627953     0.0  0.000000 -0.355113   \n",
       "2023-02 -0.994578 -0.996758 -0.741725 -0.357666     0.0  0.762566  0.157941   \n",
       "2023-03 -0.994578 -0.996758 -0.741725  0.934315     0.0  0.000000  0.121096   \n",
       "2023-04 -0.994578 -0.996758 -0.741725 -0.591908     0.0 -0.691856 -0.240231   \n",
       "\n",
       "topic     15_diff   19_diff   25_diff   27_diff  28_diff   29_diff  \n",
       "date                                                                \n",
       "2011-06       NaN       NaN       NaN       NaN      NaN       NaN  \n",
       "2011-07 -0.011141  0.521472  0.000000 -0.022846      0.0 -0.428207  \n",
       "2011-08 -0.029113 -0.230914  0.000000 -0.507192      0.0  0.840044  \n",
       "2011-09 -0.196663  0.430162  0.000000  0.000000      0.0 -0.187861  \n",
       "2011-10  0.556933 -0.386351 -1.114613  0.788709      0.0 -0.303192  \n",
       "...           ...       ...       ...       ...      ...       ...  \n",
       "2022-12  0.069210  0.000000  0.000000  0.000000      0.0  1.568919  \n",
       "2023-01  1.857642  0.001086  0.000000  0.000000      0.0  0.000000  \n",
       "2023-02  0.006886  0.000000  0.000000  0.000000      0.0 -1.350417  \n",
       "2023-03 -1.928941  0.053098  0.000000  0.000000      0.0  0.000000  \n",
       "2023-04  0.065398  1.876793  0.000000  0.000000      0.0  0.000000  \n",
       "\n",
       "[143 rows x 20 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# monthly_sentiments = df_newtopics.groupby([df_newtopics['date'].dt.to_period('M'), 'topic'])['sentiment'].mean().unstack()\n",
    "# monthly_sentiments[2].iloc[0]=monthly_sentiments[2].mean()\n",
    "# monthly_sentiments[15].iloc[0]=monthly_sentiments[15].mean()\n",
    "# monthly_sentiments[19].iloc[0]=monthly_sentiments[19].mean()\n",
    "# monthly_sentiments[25].iloc[0]=monthly_sentiments[25].mean()\n",
    "# monthly_sentiments[27].iloc[0]=monthly_sentiments[27].mean()\n",
    "# monthly_sentiments[28].iloc[0]=monthly_sentiments[28].mean()\n",
    "# monthly_sentiments[29].iloc[0]=monthly_sentiments[29].mean()\n",
    "# monthly_sentiments=monthly_sentiments.ffill()\n",
    "\n",
    "\n",
    "# monthly_sentiments['1_diff']=monthly_sentiments[1].diff()\n",
    "# monthly_sentiments['2_diff']=monthly_sentiments[2].diff()\n",
    "# monthly_sentiments['6_diff']=monthly_sentiments[6].diff()\n",
    "# monthly_sentiments['14_diff']=monthly_sentiments[14].diff()\n",
    "\n",
    "# monthly_sentiments['15_diff']=monthly_sentiments[15].diff()\n",
    "# monthly_sentiments['19_diff']=monthly_sentiments[19].diff()\n",
    "# monthly_sentiments['25_diff']=monthly_sentiments[25].diff()\n",
    "# monthly_sentiments['27_diff']=monthly_sentiments[27].diff()\n",
    "# monthly_sentiments['28_diff']=monthly_sentiments[28].diff()\n",
    "# monthly_sentiments['29_diff']=monthly_sentiments[29].diff()\n",
    "\n",
    "# #monthly_sentiments=monthly_sentiments.drop(['2_diff'],axis=1)\n",
    "\n",
    "# monthly_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_sentiments.to_csv(\"data/monthly_sentiment.csv\", index=False) # Save DataFrame to monthly_sentiment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_newtopics_copy = df_newtopics.copy()\n",
    "# df_newtopics_copy['yyyy-mm'] = pd.to_datetime(df_newtopics_copy['date']).dt.strftime('%Y-%m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_list=[]\n",
    "# for i in range(len(df_newtopics_copy)):\n",
    "#     diff_list.append(monthly_sentiments[df_newtopics_copy['topic'][i]][df_newtopics_copy['yyyy-mm'][i]])\n",
    "# df_newtopics_copy['difference']=diff_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the final sentiment csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_newtopics_copy.to_csv(\"data/topic_sentiment.csv\", index=False) # Save DataFrame to monthly_sentiment.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
